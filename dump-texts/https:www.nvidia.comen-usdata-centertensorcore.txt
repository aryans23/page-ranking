NVIDIAMy Account
  <span class="nav-profile-logo pull-left"
							title='My Account'></span>  <span class="nav-profile-text pull-left hide">sign in</span>AI and Deep LearningAutonomous MachinesData CenterDESIGN & PRO VISUALIZATIONHEALTHCARESelf-Driving CarsGeForce GamingSHIELDDevelopersCommunityShopDriversSupportAbout NVIDIAView All ProductsAI Computing ModelNVIDIA BlogCommunityCareersVIRTUAL REALITYProductsOverviewTeslaOverviewT4Tesla V100Tesla P100NVIDIA DGXOverviewDGX StationDGX-1DGX-2NVIDIA HGXNVIDIA GPU CloudOverviewDeep Learning ContainersHPC APP CONTAINERSHPC VIS CONTAINERSSolutionsOverviewAI and Deep LearningHigh Performance ComputingGPU Cloud ComputingGPU VirtualizationAnalyticsGPU Apps DirectoryGPU Ready AppsFor DevelopersNVIDIA VoltaNVIDIA PascalNVLINK/NVSWITCHTensor CoresINDEX PARAVIEW PLUGINTeslaNVIDIA DGXNVIDIA HGXNVIDIA GPU CloudAI and Deep LearningHigh Performance ComputingGPU Cloud ComputingGPU VirtualizationAnalyticsGPU Apps DirectoryGPU Ready AppsFor DevelopersNVIDIA VoltaNVIDIA PascalNVLINK/NVSWITCHTensor CoresINDEX PARAVIEW PLUGINProductsProductsOverviewTeslaTeslaOverviewT4Tesla V100Tesla P100NVIDIA DGXNVIDIA DGXOverviewDGX StationDGX-1DGX-2NVIDIA HGXNVIDIA GPU CloudNVIDIA GPU CloudOverviewDeep Learning ContainersHPC APP CONTAINERSHPC VIS CONTAINERSSolutionsSolutionsOverviewAI and Deep LearningHigh Performance ComputingGPU Cloud ComputingGPU VirtualizationAnalyticsAppsAppsGPU Apps DirectoryGPU Ready AppsFor DevelopersTechnologiesTechnologiesNVIDIA VoltaNVIDIA PascalNVLINK/NVSWITCHTensor CoresINDEX PARAVIEW PLUGINTensor Coresinstructions how to enable JavaScript in your web browser.NVIDIA TENSOR CORESThe Next Generation of Deep LearningNVIDIA® Tesla® GPUs are powered by Tensor Cores, a revolutionary technology that delivers groundbreaking AI performance. Tensor Cores can accelerate large matrix operations, which are at the heart of AI, and  perform mixed-precision matrix multiply and accumulate calculations in a single operation. With hundreds of Tensor Cores operating in parallel in one NVIDIA GPU, this enables massive increases in throughput and efficiencyBREAKTHROUGH INFERENCE PERFORMANCETesla T4 introduces NVIDIA Turing Tensor Core technology with multi-precision computing for the world’s most efficient AI inference. Turing Tensor Cores provide a full range of precisions for inference, from FP32 to FP16 to INT8, as well as INT4, to provide giant leaps in performance over NVIDIA Pascal® GPUs.THE MOST ADVANCED INFERENCE PLATFORMT4 delivers breakthrough performance for deep learning training in FP32, FP16, INT8, INT4, and binary precisions for inference. With 130 teraOPS (TOPS) of INT8 and 260TOPS of INT4, T4 has the world’s highest inference efficiency, up to 40X higher performance compared to CPUs with just 60 percent of the power consumption. Using just 75 watts (W), it’s the ideal solution for scale-out servers at the edge.Resnet50DeepSpeech2GNMTChip-to-chip GPU-to-CPU speedups | NVIDIA Tesla T4 GPU vs Xeon Gold 6140 CPUTHE WORLD’S HIGHEST DEEP LEARNING THROUGHPUTDesigned specifically for deep learning, the first-generation Tensor Cores in Volta deliver groundbreaking performance with mixed-precision matrix multiply in FP16 and FP32—up to 12X higher peak teraflops (TFLOPS) for training and 6X higher peak TFLOPS for inference over the prior-generation NVIDIA Pascal™. This key capability enables Volta to deliver 3X performance speedups in training and inference over Pascal.Each of Tesla V100's 640 Tensor Cores operates on a 4x4 matrix, and their associated data paths are custom-designed to power the world’s fastest floating-point compute throughput with high-energy efficiency.A BREAKTHROUGH IN TRAINING AND INFERENCEVolta is equipped with 640 Tensor Cores, each performing 64 floating-point fused-multiply-add (FMA) operations per clock. That delivers up to 125 TFLOPS for training and inference applications. This means that developers can run deep learning training using a mixed precision of FP16 compute with FP32 accumulate, achieving both a 3X speedup over the previous generation and convergence to a network’s expected accuracy levels.This 3X speedup in performance is a key breakthrough of Tensor Core technology. Now, deep learning can happen in mere hours.For inference, Tesla V100 also achieves more than a 3X performance advantage versus the previous generation and is 47X faster than a CPU-based server. Using the NVIDIA TensorRT™ Programmable Inference Accelerator, these speedups are due in large part to Tensor Cores accelerating inference work using mixed precisionRead the whitepaper about Tensor Cores and the NVIDIA Volta architecture.DOWNLOAD NOWNVIDIA T4NVIDIA Tesla V100NVIDIA Tesla P100NVIDIA Tesla P4/P40NVIDIA DGX SystemsNVIDIA DGX StationNVIDIA DGX-1NVIDIA DGX-2NVIDIA HGXNVIDIA GPU CloudNVIDIA VoltaNVIDIA PascalNVLink/NVSwitchTensor CoresIndeX ParaView PluginData Center BlogsGPU-Ready App Quick Start GuidesGPU Apps CatalogTesla Product LiteratureGPU Test DriveWhere to Buy - DGXWhere to Buy - TeslaQualified Server CatalogSubscribeGet the Latest from NVIDIA
  on Data CenterLIMITED TIME OFFER: $49,900 ON NVIDIA DGX STATIONFacebookTwitterLinkedInYouTube<img class="global-footer__region__icon" src="/etc/designs/nvidiaGDC/clientlibs_base/images/country-selector/us.png" />

		           USA - United StatesPrivacy PolicyLegal InfoContact Us