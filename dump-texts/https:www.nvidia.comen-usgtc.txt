NVIDIAMy Account
  <span class="nav-profile-logo pull-left"
							title='My Account'></span>  <span class="nav-profile-text pull-left hide">sign in</span>AI and Deep LearningAutonomous MachinesData CenterDESIGN & PRO VISUALIZATIONHEALTHCARESelf-Driving CarsGeForce GamingSHIELDDevelopersCommunityShopDriversSupportAbout NVIDIAView All ProductsAI Computing ModelNVIDIA BlogCommunityCareersVIRTUAL REALITYTopicsTrainingAI StartupsConnect & LearnWhy AttendPricingTravelCall for SubmissionsTalksPostersInstructor-Led CoursesWhy Sponsor and ExhibitCode of ConductInclusionPoster GalleryGTC On DemandNVIDIA WebinarsContactTopicsTrainingAI StartupsConnect & LearnWhy AttendPricingTravelCall for SubmissionsTalksPostersInstructor-Led CoursesWhy Sponsor and ExhibitCode of ConductInclusionPoster GalleryGTC On DemandNVIDIA WebinarsContactAgendaAgendaTopicsTrainingAI StartupsConnect & LearnAttendAttendWhy AttendPricingTravelPresentPresentCall for SubmissionsTalksPostersInstructor-Led CoursesExhibitExhibitWhy Sponsor and ExhibitMoreMoreCode of ConductInclusionPoster GalleryGTC On DemandNVIDIA WebinarsContactREGISTER NOWREGISTER NOWinstructions how to enable JavaScript in your web browser.GTC 2019 REGISTRATION IS NOW OPENHundreds of new AI sessions to be added in 2019.GTC 2019 REGISTRATION IS NOW OPENHundreds of new AI sessions to be added in 2019.WORKSHOPS MARCH 17, 2019 CONFERENCE MARCH 18-21, 2019Limited-time GTC Alumni rates now available.VIEW PRICINGLimited-time GTC Alumni rates now available.VIEW PRICINGJOIN THE PREMIER AI CONFERENCECONNECTInteract with technology experts from NVIDIA and other leading organizations.LEARNGet valuable insight and hands-on training through hundreds of courses, talks, and research posters.See how GPU technologies are creating amazing breakthroughs in important fields such as deep learning.Hear about disruptive innovations as early-stage companies and startups present their work.GTC is the show to be at this year if you want to see what is coming for the next generation of ever-smarter technology.- Rob Enderle, TGDailyIf GPUs are time machines, then GTC is where you come see the future.– Jensen Huang, NVIDIAIf you are interested in AI and ML, GTC is the place to be whether you are a developer, a data scientist or a business person.- Patrick Moorhead, Forbes2019 TOPICSDetailed pages on each topic coming soon.2019 INDUSTRIESDetailed  pages on each topic coming soon2019 SPEAKERSAdditional speakers to be added.JENSEN HUANGNVIDIA Founder and CEOOpening KeynotePIETER ABBEELUC BERKELEY / OPEN AI / GRADESCOPE ProfessorDeep Learning for RoboticsELLEN DUTHE DOW CHEMICAL COMPANY  Research ScientistMolecular Generative VAEs: Parallelization, Optimization, and Latent Space Analysis on the DGX-1GREG DIAMOSBAIDU Research LeadReaching Beyond Human Accuracy With AI DatacentersMARCO SCHREYERGERMAN RESEARCH CENTER FOR ARTIFICIAL INTELLIGENCE  ResearcherCreation of Adversarial Accounting Records to Attack Financial Statement AuditsRIMA ARNAOUTUNIVERSITY OF CALIFORNIA, SAN FRANCISCO Assistant ProfessorNext-Generation Diagnostics for Cardiovascular DiseaseMAGNUS HYTTSTENGOOGLE Developer AdvocateGetting Started with Machine Learning on GPUsBRYAN CATANZARONVIDIA VP of Applied Deep Learning ResearchApplied Deep Learning Research at NVIDIADAMIEN FAGNOUTECHNICOLOR SVP Technology & Infrastructure, Production ServicesGenesis: Real-Time Raytracing in Virtual ProductionDANIEL RUBINSTANFORD UNIVERSITY Professor of Biomedical Data Science, Radiology, and MedicineFrontiers of AI in Medicine: Overcoming Current Challenges and Moving Beyond ClassificationLIZY JOHNUNIVERSITY OF TEXAS B. N. Gafford ProfessorDemystifying Deep Learning Infrastructure Choices using MLPerf Benchmark SuiteWINSTON HSUNATIONAL TAIWAN UNIVERSITY ProfessorFace Recognition: from Scientific Research to Commercial ProductsPETER DECREMCITI DirectorUsing AI Machine Learning to Explore Large Streaming Financial Data Sets to Improve Market MakingFANGCHANG MAMASSACHUSETTS INSTITUTE OF TECHNOLOGY Ph.D. CandidateSparse-to-Dense: Self-supervised Depth Completion from LiDAR and Monocular CameraAVANTI SHRIKUMARSTANFORD UNIVERSITY PhD StudentUnderstanding Genome Regulation with Interpretable Deep LearningNEIL TENENHOLTZMGH & BWH CENTER FOR CLINICAL DATA SCIENCE  Director of Machine LearningScalable Development and Deployment of Machine Learning Algorithms in a Clinical SettingBENJAMIN HERNANDEZOAK RIDGE NATIONAL LABORATORY Computer ScientistGPU enhanced collaborative scientific visualizationJOHN ROMEINASTRON (NETHERLANDS INSTITUTE FOR RADIO ASTRONOMY) Senior ResearcherExtreme Signal-Processing Performance using Tensor CoresSTEPHEN JONESNVIDIA Principal Software EngineerCUDA - New Features and BeyondSHUTAO SONGTENCENT TECHNOLOGY(BEIJING) CO., LTD Senior Software EngineerTraining ImageNet in Four MinutesRICHARD HUDDLESTONMORGAN STANLEY Executive DirectorAccelerated Deep Learning Applied to Algorithmic Trading: Some Lessons LearnedTRISTA CHENINVENTEC CORPORATION Chief Scientist of Machine LearningEdge AI in Smart Manufacturing: Defect Detection and BeyondTHORSTEN KURTHLAWRENCE BERKELEY NATIONAL LABORATORY Application Performance SpecialistExascale Deep Learning for Climate AnalyticsLUCAS AMMANNABVENT / TWINMOTION R&D EngineerComplementarity between GPU Real-Time Rendering and GPU Pathtracing for Architectural VisualizationOLIVIER BREULEUXMILA Computer AnalystDeep Learning with MyiaCARL CASENVIDIA Senior ArchitectMixed Precision Training of Deep Neural NetworksWES ARMOUROXFORD ERESEARCH CENTER, DEPARTMENT OF ENGINEERING SCIENCE, UNIVERSITY OF OXFORD  DirectorAstroAccelerate - GPU enabled Signal Processing on the path to the Square Kilometre ArrayMATTHEW COOKBMW GROUP Data ScientistReal Time Vehicle Inspection With AI Aided Computer VisionLEX FRIDMANMIT Research ScientistHuman-Centered Autonomous VehicleJIRI KRAUSNVIDIA Senior Devtech ComputeMulti GPU Programming ModelsDANIEL EGLOFFFLINK AI CEO and Head of R&DDeep Reinforcement Applications in Finance with Streaming Event DataJIQUN TUCOLUMBIA UNIVERSITY Graduate StudentLattice QCD with Tensor CoresCARLOS ESCAPAAWS Global Lead, AI/ML Consulting PracticeFraming Business Problems As Machine Learning ProblemsJEFF LARKINNVIDIA Senior DevTech Software EngineerZero to GPU Hero with OpenACCJOHN STONEUNIVERSITY OF ILLINOIS AT URBANA-CHAMPAIGN Senior Research ProgrammerBringing State-of-the-Art GPU Accelerated Molecular Modeling Tools to the Research CommunitySpecial OffersALUMNI SAVINGS Past attendees at any global GTC receive 40% off regular rates through January 18, 2019.ACADEMIC, GOVERNMENT & NON-PROFIT SAVINGS Receive a 50% discount when you register with a university, government, or non-profit email address. Must present credentials at check-in to receive badge.GROUP SAVINGS Purchase 5 or more of the same pass type to receive a 25% discount when you register.VIEW PRICING1. Conference materials include access to post event recordings and presentation files, and a GTC-branded bag and t-shirt.* Discount pricing available for educational institutions, government agencies and other non-profit organizations. Must register with government or university issued email address to qualify, and present credentials at check-in to receive badge.2018 SPONSORSInterested in sponsoring, exhibiting or partnering at GTC 2019? Contact us for details.Contact us1. Conference materials include access to post event recordings and presentation files, and a GTC-branded bag and t-shirt.JOIN THE NVIDIA DEVELOPER PROGRAMAccess everything you need to develop with NVIDIA products.Register NowCHECK OUT ALL THE GREAT MOMENTS FROM GTC 2018CHECK OUT ALL THE GREAT MOMENTS FROM GTC 2018BROWSE FLICKR PHOTOSPrerequisites: Basic experience with CNNs and C++ Abstract: Smart cities collect huge amounts of video footage that require advanced deep learning techniques to transform data into actionable insights. The first step in more complex deep learning workflows is detecting specific types of objects. This involves identification, classification, segmentation, prediction, and recommendation. In this workshop, you’ll learn how to:Upon completion, you’ll be able to deploy object detection and tracking networks to work on real-time, large-scale video streams.Prerequisites: Basic experience with Neural Networks Abstract: Learn the latest deep learning techniques to understand textual input using natural language processing (NLP). You’ll learn how to:Upon completion, you’ll be proficient in NLP using embeddings in similar applications.Prerequisites: Experience with stochastic gradient descent mechanics Abstract: The computational requirements of deep neural networks used to enable AI applications like self-driving cars are enormous. A single training cycle can take weeks on a single GPU, or even years for the larger datasets like those used in self-driving car research. Using multiple GPUs for deep learning can significantly shorten the time required to train lots of data, making solving complex problems with deep learning feasible. This workshop will teach you how to use multiple GPUs to training neural networks. You'll learn:Upon completion, you'll be able to effectively parallelize training of deep neural networks using TensorFlow.Prerequisites: Basic experience with Python and NumPy Abstract: This workshop explores how to use Numba—the just-in-time, type-specializing Python function compiler—to accelerate Python programs to run on massively parallel NVIDIA GPUs. You’ll learn how to:Upon completion, you’ll be able to use Numba to compile and launch CUDA kernels to accelerate your Python applications on NVIDIA GPUs.Prerequisites: Basic experience with CNNs, circuits, and hardware. Certification available: Explore how to create robotic solutions on an NVIDIA Jetson for embedded applications by applying computer vision models to perform detection and training a robot to actuate the correct output.NVIDIA, Founder and CEOOpening KeynoteDon't miss this keynote from NVIDIA Founder & CEO, Jensen Huang, as he speaks on the future of computing.ABOUT THE SPEAKER: Jensen Huang co-founded NVIDIA in 1993 and has served since its inception as president, chief executive officer and a member of the board of directors. Under his leadership, NVIDIA invented the graphics processing unit (GPU) in 1999. Since then, it has consistently set new standards in visual computing with breathtaking, interactive graphics available on devices ranging from smartphones and tablets to notebooks and workstations. NVIDIA's expertise in programmable GPUs has led to breakthroughs in parallel processing that make supercomputing inexpensive and widely accessible. The company holds more than 7,000 U.S. patents granted or pending, including ones covering designs and insights fundamental to modern computing. Huang is a recipient of the Dr. Morris Chang Exemplary Leadership Award from the Global Semiconductor Association in recognition of his exceptional contributions to driving the development, innovation, growth and long-term opportunities of the fabless semiconductor industry. He has received the Daniel J. Epstein Engineering Management Award from the University of Southern California and an honorary doctorate from Oregon State University. He was named to the U.S. Immigrant Entrepreneur Hall of Fame when it was established in 2012. Prior to founding NVIDIA, Huang worked at LSI Logic and Advanced Micro Devices. He holds a BSEE degree from Oregon State University and an MSEE degree from Stanford University.NVIDIA, Senior Research ScientistTHE FUTURE OF AI FOR MEDIA & ENTERTAINMENTAI has already had a major impact on Media & Entertainment – from connecting people with relevant content, to video analytics and dynamic distribution. Join our panelists to gain high-level insights about new ways AI will impact the Film, Television, AR/VR, and Broadcast industries. We'll discuss advancements in content creation, dynamic delivery, and intelligent interactivity.ABOUT THE SPEAKER: Shalini De Mello is a senior research scientist at NVIDIA Research, with interests in computer vision and machine learning for human-computer interaction and smart interfaces. Her research has been incorporated into NVIDIA's shipping products for gaze, 2D, and 3D head pose estimation, hand gesture recognition, face detection, video stabilization, and GPU-optimized libraries for the development for computer vision applications on mobile platforms. Shalini received both her Ph.D. and master's degree in electrical and computer engineering from the University of Texas at Austin. Outside of work, she likes to cook, travel, and read, and take hikes with her dog.NVIDIA, Chief Technologist GPU ComputingABOUT THE SPEAKER: Mark Harris is chief technologist for GPU Computing Software at NVIDIA. Mark has 15 years of experience developing software for GPUs, ranging from graphics and games to physically based simulation, parallel algorithms, and high performance computing. Mark has been using GPUs for general-purpose computing since before they even supported floating point arithmetic. While a Ph.D. student at the University of North Carolina, he recognized this nascent trend and coined a name for it: GPGPU (general-purpose computing on graphics processing units), and started GPGPU.org to provide a forum for those working in the field to share and discuss their work.Training AI agents that can successfully generalize requires large amounts of diverse labeled training data. Collecting and labeling data is a significant cost in the development of AI applications, which, in some cases, may not even be feasible. We'll describe computer graphics facial models that we are developing to generate large labeled synthetic facial data for training deep neural networks. Facial analysis is central to many vision applications that involve human-computer interaction, including robotics, autonomous cars, rehabilitation, and extended usability. Generating and animating human faces with high realism is a well-studied problem in computer graphics; however, very few computer vision AI techniques take advantage of rendered facial data to augment or replace manually collected training data. We'll share key insights of how we successfully use synthetic facial data for training facial analysis classifiers. We'll also demonstrate many sub-tasks on which synthetic data helps to significantly improve accuracy and reduces the need for manual data collection.Shalini De Mello is a senior research scientist at NVIDIA Research, with interests in computer vision and machine learning for human-computer interaction and smart interfaces. Her research has been incorporated into NVIDIA's shipping products for gaze, 2D, and 3D head pose estimation, hand gesture recognition, face detection, video stabilization, and GPU-optimized libraries for the development for computer vision applications on mobile platforms. Shalini received both her Ph.D. and master's degree in electrical and computer engineering from the University of Texas at Austin. Outside of work, she likes to cook, travel, and read, and take hikes with her dog.NVIDIA, Director, Developer ProgramsDeep Learning DemystifiedDeep Learning DemystifiedABOUT THE SPEAKER: Will Ramey is director of developer programs at NVIDIA. Since joining the company in 2003 he has served in a number of marketing and management roles, including several years as the first product manager for CUDA and NVIDIA's developer tools. Currently Will leads teams responsible for promoting the NVIDIA SDK, developer programs, and the Deep Learning Institute. Prior to joining NVIDIA, he managed an independent game studio and developed advanced technology for the entertainment industry as a product manager and software engineer. He holds a BA in computer science from Willamette University and completed the Japan Studies Program at Tokyo International University. Outside of work Will enjoys the outdoors, learning Spanish, and open water swimming. Follow @wramey on Twitter to keep up with Will online.@wrameyNVIDIA, VP of Applied Deep Learning ResearchDeep Learning Applications in Text and Graphics at NVIDIAAt NVIDIA, we're busy applying deep learning to diverse problems, and this talk will give an overview of a few of these applications. We'll discuss our resume matching system, which helps match candidates to job openings at NVIDIA, as well as an open-source sentiment analysis project trained on unsupervised text that is improving our marketing capabilities. We'll discuss a blind image quality metric that we're using to lower the cost of raytracing photorealistic graphics, and a generative model that we've built to create realistic graphics from simplistic sketches.ABOUT THE SPEAKER: Bryan leads applied deep learning research at NVIDIA.GOOGLE, Developer Advocate, Google CloudBigQuery and TensorFlow: Data Warehouse + Machine Learning Enables the "Smart" QueryBigQuery is Google's fully managed, petabyte-scale data warehouse. It's user-defined function realizes "smart" queries with the power of machine learning, such as similarity search or recommendation on images or documents with feature vectors and neural network prediction. We'll see how TensorFlow and its GPU-accelerated training environment enables a powerful "data warehouse + machine learning" solution.ABOUT THE SPEAKER: Kaz Sato is a staff developer advocate on the Google Cloud team, focusing on machine learning and data analytics products, such as TensorFlow, Cloud ML, and BigQuery. For over eight years, Kaz has been speaking at major events, such as Google Cloud Next SF, Google I/O, Strata NYC, etc.; authoring many Google Cloud Platform blog posts; and leading developer communities for Google Cloud. He is also interested in hardwares and IoT, and hosts FPGA meetups.NVIDIA, Software & Systems ArchitectCUDA - New Features and BeyondCUDA is NVIDIA's parallel computing platform and programming model. You'll learn about new programming model enhancements and performance improvements in the latest release of CUDA, preview upcoming GPU programming technology, and gain insight into the philosophy driving the development of CUDA and how it will take advantage of current and future GPUs. You'll also learn about NVIDIA's vision for CUDA and the challenges for the future of parallel software development.ABOUT THE SPEAKER: Stephen Jones is a principal software engineer in the CUDA group at NVIDIA, working on making the CUDA language and programming model span the needs of parallel programming from high performance computing to artificial intelligence. Prior to NVIDIA he led the Simulation & Analytics group at SpaceX, where he worked on various projects including large-scale simulation of combustion processes in rocket engines. His background is in computational fluid mechanics and plasma physics, but he has worked in diverse industries including networking, CAD/CAM, and scientific computing.FACEBOOK, AI Resarch EngineerPyTorch: A Framework for Fast, Dynamic Deep Learning and Scientific ComputingPyTorch, a fast and flexible deep learning framework, has been called a 'breath of fresh air' by researchers and developers alike for ease of use, flexibility and similarity to python programming. It consists of an ndarray library that natively supports GPU execution, an automatic differentiation engine that is flexible and fast, and an optimization package for gradient based optimization methods. Come learn directly from the creator of the project, Soumith Chintala, on how you can get started today with PyTorch.ABOUT THE SPEAKER: Soumith Chintala is an engineer at Facebook AI Research, where he works on deep learning and unsupervised learning. Soumith's work on adversarial networks and Torch has been featured in GTC keynotes by Jensen Huang.MONTREAL INSTITUTE FOR LEARNING ALGORITHMS, Post-Doctoral ResearcherLearning Normalized Inputs for Iterative Estimation in Medical Image SegmentationIn medical imaging, acquisition procedures and imaging signals vary across different modalities and, thus, researchers often treat them independently, introducing different models for each imaging modality. To mitigate the number of modality-specific designs, we introduced a simple yet powerful pipeline for medical image segmentation that combines fully convolutional networks (FCNs) with fully convolutional residual networks (FC-ResNets). FCNs are used to obtain normalized images, which are then iteratively refined by means of a FC-ResNet to generate a segmentation prediction. We'll show results that highlight the potential of the proposed pipeline, by matching state-of-the-art performance on a variety of medical imaging modalities, including electron microscopy, computed tomography, and magnetic resonance imaging.ABOUT THE SPEAKER: Michal Drozdzal is a postdoctoral researcher at the Montreal Institute for Learning Algorithms. Michal’s research interests focus on designing machine learning approaches to tackle impactful problems present in medical imaging, including topics such as image segmentation, unsupervised learning, or learning from small amount of labeled data. Previously, he spent one year as postdoctoral researcher in Medtronic GI, working under the umbrella of Neurogut project, and one year at Polytechnique of Montreal, designing machine learning approaches for medical data analysis. Michal also collaborates with Imagia, a Montreal-based startup. He received his Ph.D. from the University of Barcelona, with a thesis on computer vision approaches to automation and discovery using wireless capsule endoscopy data.ADOBE RESEARCH, Research ScientistLearning-Free Universal Style TransformerUniversal style transfer aims to transfer any arbitrary visual styles to content images. Existing feed-forward based methods, while enjoying the inference efficiency, are mainly limited by inability of generalizing to unseen styles or compromised visual quality. We'll present a simple yet effective method that tackles these limitations without training on any pre-defined styles. The key ingredient of our method is a pair of feature transform -- whitening and coloring -- that are embedded to an image reconstruction network. The whitening and coloring transforms reflect a direct matching of feature covariance of the content image to a given style image, which shares similar spirits with the optimization of Gram matrix-based cost in neural style transfer. We demonstrate the effectiveness of our algorithm by generating high-quality stylized images with comparisons to a number of recent methods. We also analyze our method by visualizing the whitened features and synthesizing textures via simple feature coloring.ABOUT THE SPEAKER: Chen Fang is a research scientist at Adobe Research. His research interests are computer vision and machine learning.OTOY, CEOLight Field Rendering and Streaming for VR and ARReal-Time Raytracing and RNDR: The Path to Holographic MediaLight Field Rendering and Streaming for VR and AR We'll discuss OTOY's cutting-edge light field rendering toolset and platform, which allows for immersive experiences on mobile HMDs and next-gen displays, making it ideal for VR and AR. OTOY is developing a groundbreaking light field rendering pipeline, including the world's first portable 360 LightStage capture system and a cloud-based graphics platform for creating and streaming light field media for VR and emerging holographic displays.Real-Time Raytracing and RNDR: The Path to Holographic Media Join OTOY Co-Founder and CEO Jules Urbach as he discusses the latest trends in light field display and delivery ecosystems. This includes recent work within MPEG to establish global standards for holographic media and light field rendering that connect to OTOY's ongoing Unity and Facebook 6DOF integrations. Jules will also present a dive deep into OTOY's RNDR network, discussing the goals and challenges of deploying a fully decentralized GPU ray tracing and streaming platform on top of the Ethereum blockchain. The talk will cover lessons learned from industry partners and stakeholders, and how future phases of RNDR will unlock modular IP rights management of holographic media and AR services through the blockchain.ABOUT THE SPEAKER: Jules Urbach is a pioneer in computer graphics, streaming, and 3D rendering with over 25 years of industry experience. He made his first game, Hell Cab (Time Warner Interactive), at age 18, which was one of the first CD-ROM games ever created. Six years later, Jules founded Groove Alliance, which created the first 3D game ever available on Shockwave.com (Real Pool). Currently, Jules is busy working on his two latest ventures, OTOY and LightStage, which aim to revolutionize 3D content capture, creation, and delivery.UNIVERSITY OF TORONTO, Assistant ProfessorTeaching Machines to See, Communicate, and ActA successful autonomous system needs to not only understand the visual world but also communicate its understanding with humans. To make this possible, language can serve as a natural link between high level semantic concepts and low level visual perception. We'll discuss recent work in the domain of vision and language, covering topics such as image/video captioning and retrieval, and question-answering. We'll also talk about our recent work on task execution via language instructions.ABOUT THE SPEAKER: Sanja Fidler is an Assistant Professor at the Department of Computer Science, University of Toronto.  Previously she was a Research Assistant Professor at TTI-Chicago, a philanthropically endowed academic institute located in the campus of the University of Chicago. She completed her PhD in computer science at  University of Ljubljana in 2010, and was a postdoctoral fellow at University of Toronto during 2011-2012. In 2010 she visited UC Berkeley. She has served as a Program Chair of the 3DV conference, and as an Area Chair of CVPR, ICCV, EMNLP, ICLR, and NIPS. She received the NVIDIA Pioneer of AI award. Her recent work on semi-automatic object labeling won the Best Paper Honorable mention at CVPR'17. Her main research interests are object detection, 3D scene understanding, and the intersection of language and vision.BMW GROUP, Data ScientistBeyond Autonomous Driving: Unleashing Value via Machine Learning Applications in Automotive IndustryData-driven applications based on machine and deep learning are striking roots in the automotive industry. In our talk, we will explore machine learning use cases next to one of the core strategic topics autonomous driving at the BMW Group. We employ novel machine and deep learning pipelines, such as those based on XGBoost and convolutional neural nets, to support a broad range of business requirements. Our setups have proven to be effective in creating value for domains including, but not limited to, vehicle engineering and after-sales. Subsequently, we will outline best practices and lessons learned from both an architectural and methodological perspective.ABOUT THE SPEAKER: Arpit Mehta is a data scientist and product owner for big data architecture at BMW Group. He is a machine learning, deep learning, and AI enthusiast, and works to bring them together to solve different business challenges. Arpit is also interested in evaluating newer technologies and defining new strategies for architecting data-driven production use cases.COGNATA, CEODeep Learning Autonomous Driving SimulationRealistic automotive simulation platforms, where virtual cars travel virtual roads in virtual cities in remarkably true-to-life conditions, will be a vital part of developing and testing autonomous vehicles. The technology behind the Cognata simulation engine is heavily based on deep learning, computer vision, and other advanced AI methods. We'll present a cloud-based simulation engine, and discuss how it works and how to develop with it.ABOUT THE SPEAKER: Danny Atsmon, an expert in advanced driver-assistance systems (ADAS) and deep learning, is the CEO at Cognata Ltd. He has been in the business of launching high-tech products for more than 20 years. Before joining Cognata, Danny served as HARMAN’s (now Samsung) director of ADAS and senior director of machine learning. He co-founded two startups, Picitup and iOnRoad, the latter of which uses a phone's native camera to detect vehicles in front of a car. Danny also holds several patents and has won many top industry awards, including the CES award, CTIA award, Microsoft Think Next, and Qualcomm QPrize. Danny has a Bachelor of Science in physics from Tel Aviv University, and is a graduate of the Israeli Defense Forces Haman Talpiot program, where he served in the elite Unit 8200.SALESFORCE RESEARCH, Senior Research ScientistState-of-the-Art Large Scale Language Modeling in 12 Hours with a Single GPUFor sequence learning tasks that utilize recurrent neural networks, scale is both the key to accuracy and the bane of speed. We take existing state-of-the-art language modeling techniques and speed them up by orders of magnitude without losing accuracy. The tactics include injecting flexibility into NVIDIA's black box cuDNN LSTM; replacing the LSTM with the more parallelized and customizable Quasi-Recurrent Neural Network; reducing the softmax bottleneck using the adaptive softmax; and investigating individual function efficiency on the GPU using the NVIDIA Visual Profiler. The end result is a general and scalable language model framework that can achieve state-of-the-art quality on the WikiText-103 dataset (103 million words) in under 12 hours using a single NVIDIA Volta V100. The resulting PyTorch codebase is open source for experimentation and extension.ABOUT THE SPEAKER: Stephen Merity is a senior research scientist at Salesforce Research, joining as part of the MetaMind acquisition. His recent publications have included achieving state-of-the-art word level language modeling and generating novel recurrent neural networks, both by hand and through reinforcement learning. Previously, Stephen worked on big data at Common Crawl, data analytics at Freelancer.com, and online education at Grok Learning. Stephen holds a master's degree in computational science and engineering from Harvard University, and a Bachelor of Information Technology from the University of Sydney. He can be found on Twitter at @smerity.@smerityUC BERKELEY, ProfessorLearning to Learn, Deep Learning for Robotics, Deep Reinforcement Learning, AI for Manufacturing and LogisticsWe'll introduce the latest advances on topics such as learning-to-learn, meta-learning, deep learning for robotics, deep reinforcement learning, and AI for manufacturing and logistics.ABOUT THE SPEAKER: Pieter Abbeel is a professor at UC Berkeley, research scientist at OpenAI, and co-founder of Gradescope. He works in machine learning and robotics -- his research focuses on making robots learn from people, (apprenticeship learning); how to make robots learn through their own trial and error (reinforcement learning); and how to speed up skill acquisition through learning-to-learn. His robots have learned advanced helicopter aerobatics, knot-tying, basic assembly, and laundry organizing. His group has pioneered deep reinforcement learning for robotics, including learning visuomotor skills and simulated locomotion. He has won various awards, including best paper awards at ICML, NIPS, and ICRA, the Sloan Fellowship, the Air Force Office of Scientific Research Young Investigator Program award, the Office of Naval Research Young Investigator Program award, the DARPA Young Faculty Award, and the National Science Foundation Faculty Early Career Development Program Award.NVIDIA, VP, GameWorks and Lightspeed StudiosABOUT THE SPEAKER: Rev Lebaredian started his career in Hollywood, specializing in rendering for Warner Bros. Digital and Disney’s Dream Quest Images. He developed the renderer used for the Academy Award nominated film Mighty Joe Young. He then formed Steamboat Software and developed the award-winning renderer Jig, used by visual effects studios such as Digital Domain, PDI/Dream Works, Sony ImageWorks, Walt Disney, and Rhythm and Hues for various films including X-Men 2, Stuart Little, The Core, Reign of Fire, Sum of All Fears, Treasure Planet, Gone in Sixty Seconds and How the Grinch Stole Christmas. In 2002, Rev transitioned from film visual effects to NVIDIA, where he helped in the creation of the Cg shading language. NVIDIA relocated Rev to Moscow to open a new site for NVIDIA and to architect a state-of-the-art games testing and data-mining lab, as well as NVIDIA’s groundbreaking client, GeForce Experience. Today, Rev is vice president of GameWorks and Lightspeed studios at NVIDIA, responsible for developing NVIDIA GameWorks technologies, first-party games, the Holodeck VR platform, and the Isaac Lab robot simulator. GameWorks includes various visual effects modules tailored for real-time high-fidelity applications, such as NVIDIA Destruction, NVIDIA Clothing, HairWorks, WaveWorks, Ansel, as well as the most popular real-time physics middleware for video games, PhysX.Training AI agents that can successfully generalize requires large amounts of diverse labeled training data. Collecting and labeling data is a significant cost in the development of AI applications, which, in some cases, may not even be feasible. We'll describe computer graphics facial models that we are developing to generate large labeled synthetic facial data for training deep neural networks. Facial analysis is central to many vision applications that involve human-computer interaction, including robotics, autonomous cars, rehabilitation, and extended usability. Generating and animating human faces with high realism is a well-studied problem in computer graphics; however, very few computer vision AI techniques take advantage of rendered facial data to augment or replace manually collected training data. We'll share key insights of how we successfully use synthetic facial data for training facial analysis classifiers. We'll also demonstrate many sub-tasks on which synthetic data helps to significantly improve accuracy and reduces the need for manual data collection.Shalini De Mello is a senior research scientist at NVIDIA Research, with interests in computer vision and machine learning for human-computer interaction and smart interfaces. Her research has been incorporated into NVIDIA's shipping products for gaze, 2D, and 3D head pose estimation, hand gesture recognition, face detection, video stabilization, and GPU-optimized libraries for the development for computer vision applications on mobile platforms. Shalini received both her Ph.D. and master's degree in electrical and computer engineering from the University of Texas at Austin. Outside of work, she likes to cook, travel, and read, and take hikes with her dog.CHAOS GROUP, CTOInteractive and Production Rendering with V-Ray GPUCome learn the latest advances in GPU acceleration for the Academy Award winning V-Ray renderer and how it's improving artistic workflows and speeding final frame rendering.ABOUT THE SPEAKER: Vladimir Koylazov (Vlado) has more than 15 years of software development experience, the majority of which he spent developing and improving the render engine V-Ray. Passionate about 3D graphics and programming, Vlado is the driving force behind Chaos Group's software solutions. Vladimir is CTO of Chaos Software and one of the original creators of the V-Ray renderer.UNIVERSITY OF OXFORD, DEPT OF ENGINEERING, DirectorAstroAccelerate - GPU-Accelerated Signal Processing for Next Generation Radio TelescopesAstroAccelerate is a GPU-enabled software package that focuses on enabling real-time processing of time-domain radio-astronomy data. It uses the CUDA programming language for NVIDIA GPUs. The massive computational power of modern day GPUs allows the code to perform algorithms such as de-dispersion, single pulse searching, and Fourier domain acceleration searching in real time on very large datasets, which are comparable to those that will be produced by next-generation radio telescopes such as the Square Kilometre Array.ABOUT THE SPEAKER: Wes Armour is associate director at the Oxford e-Research Centre (OeRC). He leads the scientific computing group at OeRC, which has around 10 members. Wes’ grant portfolio spans a range of different funders, reflecting the interdisciplinary projects that the group is involved with. His research interests are strongly focused in scientific computing, specifically the use of HPC and many-core technologies to answer scientific problems or to impact people's daily lives. Wes’ work focuses are modeling and simulation, digital signal processing, HPC/many-core and real-time computing for big data/data science. Wes has a Master's of Physics in fundamental particle physics and cosmology and a Ph.D. in lattice gauge theory, the computational description of the strong nuclear force.TOYOTA INFOTECHNOLOGY CENTER, USA, Principal ResearcherConnected Automated Driving: Overview, Design, and Technical ChallengesWe'll discuss the important emerging field of connected automated driving, including technical and policy topics in this area. We'll provide background on vehicular safety communications and current deployments in various parts of the world. Vehicular communication will enable sensor data sharing between vehicles, which could be the key for achieving higher levels of automation. Novel artificial intelligence techniques exploiting sensor data (camera, radar, GPS etc.) from neighboring cars can be used for designing perception and mapping functionalities for automated vehicles. We'll discuss results from field testing and show advantages of connected automated driving.ABOUT THE SPEAKER: Gaurav Bansal is a principal researcher at the Toyota InfoTechnology Center in Mountain View, Calif., where he leads several research initiatives on the design of communication systems for automated driving. Gaurav is an expert in vehicular communications, pioneering contributions in dedicated short range communications (DSRC) congestion control and in innovative use-cases to leverage connectivity in cars. His research interests include cooperative perception and mapping for autonomous vehicles. Gaurav represents Toyota in the Automakers' Vehicle Safety Communication Consortium and in the SAE, ETSI standardization bodies. Gaurav's paper on DSRC congestion control received the Best Paper Award at the IEEE WiVEC Symposium. He also serves in the editorial board of IEEE Vehicular Technology Magazine and IEEE Connected Vehicles Initiative. Gaurav holds several patents, and has both a Bachelor of Technology and Ph.D. in electrical engineering from Indian Institute of Technology Kanpur and the University of British Columbia, respectively.IBM RESEARCH CHINA, Research Staff MemberUsing Multimodal Learning for TV Show SummarizationWe'll explore new techniques for TV show summarization using multimodal deep learning for saliency detection and fusion. For TV show summarization, the goal is to compact visual summary with informativeness and enjoyability to attract audience. In our work, we propose a multimodal summarization platform to integrate the multimodal saliences learned from video, audio, and text. Our work focuses on three aspects: 1) the saliency extraction for video, audio, and text using deep learning networks; 2) fusion framework design for multimodal information integration; 3) developing tools to speed up video processing. Using AI Vision, which is a public cloud-based AI service, we summarize a TV show with 11 hours duration in one minute.ABOUT THE SPEAKER: Qing Wang graduated from Electrical & Electronic Engineering School, Nanyang Technological University, Singapore in 2003 with Ph.D. After that, Qing joined IBM Research China as a research staff member and worked in leading research projects, such as wireless network cloud and Internet of Things. Since 2015, she has focused on computer vision with deep learning methods. Qing is a key contributor to the cloud-based deep learning platform called AI Vision. Her research interests include neural network auto design, multimodal learning, and medical image processing. She is an IEEE senior member, has published more than 30 papers, and has owned more than 20 patents.AMERICAN COLLEGE OF RADIOLOGY (ACR), ACR Data Science Institute Chief Science OfficerHarnessing AI: Creating a Healthcare AI EcosystemIn this session, attendees will learn how to develop an AI Learning Platform for healthcare, develop initial(imaging) AI applications in specific care areas, and embed AI into devices creating "intelligent imaging systems."ABOUT THE SPEAKER: Keith J. Dreyer, DO, Ph.D. FACR, FSIIM, is Chief Data Science Officer and Corporate Director for Enterprise Medical Imaging for Partners Healthcare. He also holds the positions of Vice Chairman of Radiology at Massachusetts General Hospital, Chief Data Science and Information Officer for the Departments of Radiology at Massachusetts General Hospital and Brigham and Women's Hospital, and Associate Professor of Radiology at the Harvard Medical School. He is ABR board certified in diagnostic radiology with a BS in Mathematics, MS in Image Processing, PhD in Computer Science and medical fellowships in Imaging Informatics and Magnetic Resonance Imaging from Harvard University at MGH. Dr. Dreyer has held numerous board, chair, advisory and committee positions with the American College of Radiology, Radiological Society of North America, Society of Imaging Informatics in Medicine and numerous global healthcare corporations. He has authored hundreds of scientific papers, presentations, chapters, articles and books; lecturing worldwide on clinical data science, cognitive computing, clinical decision support, clinical language understudying, digital imaging standards, and implications of technology on the quality of healthcare and payment reform initiatives.LAWRENCE LIVERMORE NATIONAL LABORATORY, Computer ScientistAcceleration of HPC Applications on Hybrid CPU-GPU Systems: When Can Multi-Process Service Help?When does the Multi-Process Service (MPS) improve performance of HPC codes? We aim to answer this question by exploring the effectiveness of MPS in a number of HPC applications combining distributed and shared memory parallel models. A single complex application typically includes stages with limited degree of parallelism where CPU cores are more effective than GPUs, and highly parallelizable stages acceleratable by offloading to the GPUs. MPS allows offloading computation from a number of processes to the same GPU, and, as a result, more CPU cores per node can tackle tasks characterized by limited shared memory parallelism. We demonstrate effectiveness of MPS in large-scale simulations on the IBM Minsky and Witherspoon nodes with two multi-core POWER CPUs combined with 4-6 NVIDIA GPUs.ABOUT THE SPEAKER: Olga Pearce is a computer scientist in the Center for Applied Scientific Computing (CASC) at Lawrence Livermore National Laboratory (LLNL). Her research interests include distributed data structures and parallel algorithms; parallel programing models; performance analysis, optimization, and modeling; and application load balancing. Olga applies her research by optimizing simulations on several supercomputers, including SEQUOIA. Olga joined LLNL in 2009 as a Lawrence Scholar and joined CASC in 2014. Olga received her Ph.D. in computer science in 2014 from Texas A&M University.NVIDIA, Director, Developer ProgramsDeep Learning DemystifiedDeep Learning DemystifiedABOUT THE SPEAKER: Will Ramey is director of developer programs at NVIDIA. Since joining the company in 2003 he has served in a number of marketing and management roles, including several years as the first product manager for CUDA and NVIDIA's developer tools. Currently Will leads teams responsible for promoting the NVIDIA SDK, developer programs, and the Deep Learning Institute. Prior to joining NVIDIA, he managed an independent game studio and developed advanced technology for the entertainment industry as a product manager and software engineer. He holds a BA in computer science from Willamette University and completed the Japan Studies Program at Tokyo International University. Outside of work Will enjoys the outdoors, learning Spanish, and open water swimming. Follow @wramey on Twitter to keep up with Will online.@wrameyConference SessionsTraining / Hands-on Labs(up to 3 per day)Talks and TutorialsKeynotesExhibitsLunchEvening ReceptionsGTC Party1Conference Materials2Pre-GTC DLI Workshop(Sunday, March 25)1. Additional GTC Party passes available for purchase for a spouse/guest with a paid registration. Available as add-on for Exhibits Only or One-Day (other than Wednesday, 3/28) pass holders. GTC Party pass valid for access to party on Wednesday, 3/28 only; does not include any other conference privileges.

    2. Conference materials include access to post event recordings and presentation files, and a GTC-branded bag and t-shirt.END CUE, Director, Strategy & Business DevelopmentThe Future of AI for Media & EntertainmentAI has already had a major impact on Media & Entertainment – from connecting people with relevant content, to video analytics and dynamic distribution. Join our panelists to gain high-level insights about new ways AI will impact the Film, Television, AR/VR, and Broadcast industries. We'll discuss advancements in content creation, dynamic delivery, and intelligent interactivity.ABOUT THE SPEAKER: Munika Lay joined End Cue from Paramount Pictures, where she led the Strategic Planning & Business Development group, coordinating the greenlight process and developing business plans for new media and technologies. Prior to that, Munika worked in Planning & Analysis at Warner Bros., where she identified and built out cross-departmental efficiencies. She was also an integral part of QED International, the independent film financing and international sales company behind DISTRICT 9 and FURY. She started her career at WME Entertainment in the Motion Picture Literary and Global Finance & Distribution groups. Munika hails from Long Beach, California, and she holds an MBA from USC's Marshall School of Business and a BA in Communication from Stanford University.ILMXLAB, Executive in ChargeThe Future of AI for Media & EntertainmentAI has already had a major impact on Media & Entertainment – from connecting people with relevant content, to video analytics and dynamic distribution. Join our panelists to gain high-level insights about new ways AI will impact the Film, Television, AR/VR, and Broadcast industries. We'll discuss advancements in content creation, dynamic delivery, and intelligent interactivity.ABOUT THE SPEAKER: Vicki Dobbs Beck is the Executive in Charge of ILMxLAB, a division launched in June, 2015, whose mission is to pioneer in the area of immersive storytelling. ILMxLAB wants to make it possible for people to 'Step Inside Our Stories'. The division combines the storytelling and innovation talents of Lucasfilm, ILM, and Skywalker Sound and builds on a foundation of pioneering R&D in real-time computer graphics and virtual production. Under Vicki's leadership, ILMxLAB created the ground-breaking VR installation, Carne y Arena, that was the vision of Alejandro Iñárritu in association with Legendary Entertainment and Fondazione Prada. Carne y Arena was chosen as the first-ever VR Official Selection at the Cannes Film Festival (2017) and was awarded a special Oscar by the Academy of Motion Picture Arts and Sciences "in recognition of a visionary and powerful experience in storytelling". In addition to producing multiple promotional VR experiences supporting major film releases, ILMxLAB most recently collaborated with the VOID to develop and produce the hyper-reality experience: Star Wars: Secrets of the Empire. Vicki has more than thirty years of broad-based management experience in the entertainment industry. Prior to that, she received her MBA from Stanford University's Graduate School of Business where she also completed her undergraduate studies, earning a BA with distinction in International Relations.TECHNICOLOR, SVP of Immersive MediaThe Future of AI for Media & EntertainmentAI has already had a major impact on Media & Entertainment – from connecting people with relevant content, to video analytics and dynamic distribution. Join our panelists to gain high-level insights about new ways AI will impact the Film, Television, AR/VR, and Broadcast industries. We'll discuss advancements in content creation, dynamic delivery, and intelligent interactivity.ABOUT THE SPEAKER: Marcie Jastrow is an industry veteran with over 18 years of experience in the Entertainment business. In 2016, Jastrow was appointed the SVP of Immersive Media for Technicolor, as well as the Head of the Technicolor Experience Center, a center focused on developing high-concept content, platforms and technology for virtual reality (VR), augmented reality (AR) and other immersive media applications. Prior to her new role, Jastrow served as the SVP of Sales for Technicolor Production Services, where she was responsible for growing Technicolor’s sales pipeline across theatrical and broadcast post production services for the last five years. Before Marcie joined Technicolor, she served as the EVP of Sales at Laser Pacific, which Technicolor acquired in 2011, and as SVP of Sales at Modern Video Film for over eight years. Known for her ability to bring together teams and workflows to make any size project succeed, her passion lays in bringing a creative’s vision to life.BAIDU, Tech Lead of PaddlePaddlePaddlePaddle: A Deep Learning Compiler Generating CUDA CodeWe'll discuss the performance improvement of PaddlePaddle, a deep-learning framework that describes a model as sequences of layers (open sourced in Oct 2016). By GTC 2017, it had a new core and new API (it doesn't describe models). The computation procedure and the new API is still in Python, but the AI programs written with that API has a structure that looks more like high-level programming language (C++/Java) programs. The next step is automatic code generation, in particular, CUDA code.ABOUT THE SPEAKER: Yi Wang is the Tech Lead for PaddlePaddle at Baidu.TopicsSession SchedulerTrainingAI StartupsConnect & LearnWhy AttendTravelDeveloper ZoneGTC On-DemandNVIDIA WebinarsPoster GalleryContact UsSubscribeSUBSCRIBE FOR UPDATES#GTC19FacebookTwitterLinkedInPrivacy PolicyLegal InfoContact Us